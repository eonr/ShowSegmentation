{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:27.784150Z",
     "start_time": "2019-08-22T13:14:11.087443Z"
    }
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import subprocess\n",
    "# import matplotlib\n",
    "# from matplotlib.pyplot import figure\n",
    "# from datetime import datetime\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import telegram\n",
    "\n",
    "import os\n",
    "import time as tm\n",
    "import cv2\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:27.807259Z",
     "start_time": "2019-08-22T13:14:27.789947Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadPickle(filename):\n",
    "    infile = open(filename,'rb')\n",
    "    return pickle.load(infile, encoding='latin1')\n",
    "\n",
    "def storePickle(filename, data):\n",
    "    file = open(filename,'wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:27.986102Z",
     "start_time": "2019-08-22T13:14:27.812307Z"
    }
   },
   "outputs": [],
   "source": [
    "def sec2HMS(seconds):\n",
    "    return tm.strftime('%H:%M:%S', tm.gmtime(seconds))\n",
    "\n",
    "def HMS2sec(time_str):\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## celebDetect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:57.244179Z",
     "start_time": "2019-08-22T13:37:51.940910Z"
    }
   },
   "outputs": [],
   "source": [
    "celebs, celeb_encodings = loadPickle('final_celeb_detection/final_pickles/anchors-with-TV-encodings.pickle')\n",
    "celeb_encodings = np.array([np.array(x) for x in celeb_encodings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:32:20.350402Z",
     "start_time": "2019-08-23T13:32:18.898144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populating KNN space with labelled encodings\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(len(celeb_encodings)): #prepare dataset\n",
    "    for celeb_encoding in celeb_encodings[i]:\n",
    "        X.append(celeb_encoding)\n",
    "        Y.append(celebs[i])\n",
    "        \n",
    "neigh = KNeighborsClassifier(n_neighbors=30)\n",
    "neigh.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:58.728208Z",
     "start_time": "2019-08-22T13:37:58.724315Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoding2name(f_encodings):\n",
    "        return neigh.predict(f_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:28.388285Z",
     "start_time": "2019-08-22T13:14:28.319517Z"
    }
   },
   "outputs": [],
   "source": [
    "vid_path = '../data/2006-2Hours.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:28.317248Z",
     "start_time": "2019-08-22T13:14:28.203211Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO - take input from the user - next cell as well\n",
    "output_path = '../data/output_cuts'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:38:38.356377Z",
     "start_time": "2019-08-23T13:38:38.334243Z"
    }
   },
   "outputs": [],
   "source": [
    "class Show:\n",
    "    def __init__(self, hosts, start_time, end_time):\n",
    "        self.hosts = hosts\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:28.070753Z",
     "start_time": "2019-08-22T13:14:27.996376Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEncodings(vid_path, skip_seconds=1):\n",
    "    \"\"\" Returns 129D encodings for each face present in the video\n",
    "        First 128 are the face encodings and the last value is the time.\n",
    "        \n",
    "       :param interval: (in seconds) frame interval to skip and look for faces\n",
    "       :param model: 'hog' is less accurate but faster compared to 'cnn'\n",
    "       :param store: if True, stores the faces in directory specified by dirName \"\"\"\n",
    "    vidcap = cv2.VideoCapture(vid_path)\n",
    "    cv2.VideoCapture \n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    interval = int(fps+1)*skip_seconds #no. of frames to skip\n",
    "    print(\"FPS of the video: {}\".format(fps))\n",
    "    allEncodings = [] #Dict containing path, box and encoding\n",
    "    n_frame = 0 # no. of frame being processed\n",
    "\n",
    "    success, frame = vidcap.read()\n",
    "    \n",
    "    while success:\n",
    "        rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        bboxes = face_recognition.face_locations(rgb,model='hog')\n",
    "        encodings = face_recognition.face_encodings(rgb,bboxes)\n",
    "        for i,bbox in enumerate(bboxes): #for each found face in the frame\n",
    "            top,right,bottom,left = bbox[0],bbox[1],bbox[2],bbox[3]\n",
    "            face_img = frame[top:bottom, left:right]\n",
    "            \n",
    "            d = {'time': (n_frame/fps), 'loc': bbox, 'encoding':encodings[i]}\n",
    "            allEncodings.append(d)\n",
    "            \n",
    "        n_frame += interval            \n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES,n_frame)\n",
    "        success, frame = vidcap.read()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    vidcap.release()\n",
    "    return allEncodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:14:28.198323Z",
     "start_time": "2019-08-22T13:14:28.072265Z"
    }
   },
   "outputs": [],
   "source": [
    "def clusterFaces(allEncodings, n_jobs=-1, max_clusters = 100):\n",
    "    \"\"\"face_times: a Tuple list with x[0] as face_classes, and x[1] as list of their\n",
    "    times of occurence in the video.\n",
    "    \n",
    "       face_encodings: dict mapping face_class to list of encodings of all occurences \n",
    "       of that face in the video\"\"\"\n",
    "    encodings = [d['encoding'] for d in allEncodings]\n",
    "    times = [d['time'] for d in allEncodings]\n",
    "    clt = DBSCAN(metric='euclidean', n_jobs=n_jobs, min_samples=5,eps=0.37)\n",
    "    clt.fit(encodings)\n",
    "    labels = clt.labels_ #-1 => too small to include in a cluster\n",
    "    \n",
    "    face_times = [] #list of tuples, first element is time of occurence, 2nd is the class of the face\n",
    "    face_encodings = {} #dict mapping from a face_classs to all encodings of that faces (from the images found in the video)\n",
    "    \n",
    "    for i,label in enumerate(labels):\n",
    "        encoding = encodings[i]\n",
    "        time = times[i]\n",
    "        if(label!=-1):\n",
    "            face_times.append((int(time), str(label)))\n",
    "            \n",
    "            if label in face_encodings:\n",
    "                face_encodings[label].append(encoding)\n",
    "            else:\n",
    "                face_encodings[label] = [encoding]\n",
    "    return face_times,face_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:50.082102Z",
     "start_time": "2019-08-22T13:37:50.076024Z"
    }
   },
   "outputs": [],
   "source": [
    "def addEmptyFaces(faces, skip_seconds):\n",
    "    \"\"\"Modifies faces dict to include timestamps where no faces are present\n",
    "       '-1' is the value assigned to these.\n",
    "       :skip_gap: 'interval' parameter given in file2encoding() function (in seconds)\"\"\"\n",
    "    min_time = (faces[0][0])\n",
    "    max_time = (faces[-1][0])\n",
    "    curr_time = min_time\n",
    "    faces_empty = []\n",
    "    counter = 0\n",
    "    \n",
    "    while (curr_time < max_time):\n",
    "        if((faces[counter][0]) > curr_time): #No face found at this time\n",
    "            faces_empty.append(((curr_time), '-1'))\n",
    "        else:                              #Face was already marked at this time\n",
    "            faces_empty.append(faces[counter])\n",
    "            counter+=1\n",
    "        curr_time += skip_seconds\n",
    "    return faces_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:50.221373Z",
     "start_time": "2019-08-22T13:37:50.084642Z"
    }
   },
   "outputs": [],
   "source": [
    "def faceTrendsDuration(faces, interval = 900, overlapping = False, join_consecutive = False,n_top=10):\n",
    "    \"\"\"Trendy faces are the faces of an actor which occur the most in a given interval.\n",
    "       Video is split into *interval*s and most occuring faces in them are noted.\n",
    "       For each interval, *n_top* no. of most occuring faces are returned\n",
    "       in a dict format.\"\"\"\n",
    "    #GOTO JUMPER if change interval\n",
    "    # interval - SKIP_INTERVAL*interval time duration is taken as length of one trend_bucket\n",
    "    trending_face = faces[0][1] #First face's class\n",
    "    trendy_faces = {}\n",
    "    \n",
    "    if overlapping:\n",
    "        skip=1\n",
    "    else:\n",
    "        skip=interval\n",
    "    for x in range(0, len(faces), skip):\n",
    "        face_count = {} #Keeps count of no. of instances of each face_class\n",
    "        interval_string = sec2HMS(faces[x][0])\n",
    "        for face in faces[x:min(len(faces),x+interval)]:\n",
    "            curr_time = face[0]\n",
    "            curr_face = face[1]\n",
    "            if curr_face == '-1':\n",
    "                continue\n",
    "                \n",
    "            if curr_face in face_count:\n",
    "                face_count[curr_face] = (face_count[curr_face][0],curr_time)\n",
    "            else:\n",
    "                face_count[curr_face] = (curr_time, curr_time)\n",
    "                \n",
    "        if face_count: # if face_count is not empty\n",
    "            max_face_in_interval = sorted(list(face_count.keys()), key =(lambda key: (face_count[key][1]) - (face_count[key][0])),reverse=True)[:n_top]\n",
    "        else:\n",
    "            max_face_in_interval = ['-1']\n",
    "        if join_consecutive:\n",
    "            if(max_face_in_interval!=trending_face):\n",
    "                trending_face = max_face_in_interval\n",
    "                trendy_faces[interval_string] = trending_face\n",
    "        else:\n",
    "            trending_face = max_face_in_interval\n",
    "            trendy_faces[interval_string] = trending_face\n",
    "\n",
    "#             if (face_count[curr_face]>face_count[trending_face]):\n",
    "#                 trending_face = curr_face\n",
    "#                 curr_time = face[0]\n",
    "#                 trendy_faces[curr_time] = curr_face\n",
    "    return trendy_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:50.073518Z",
     "start_time": "2019-08-22T13:14:28.399180Z"
    }
   },
   "outputs": [],
   "source": [
    "SKIP_SECONDS = 1 \n",
    "allEncodings = getEncodings(vid_path, SKIP_SECONDS)\n",
    "face_list, face_encodings = clusterFaces(allEncodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:50.325928Z",
     "start_time": "2019-08-22T13:37:50.224333Z"
    }
   },
   "outputs": [],
   "source": [
    "faces_empty = addEmptyFaces(face_list, SKIP_SECONDS)\n",
    "trends = faceTrendsDuration(faces_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:50.391339Z",
     "start_time": "2019-08-22T13:37:50.329488Z"
    }
   },
   "outputs": [],
   "source": [
    "face_dict = {} #dict having all occurences of each face\n",
    "for x in face_list: \n",
    "    if x[1] in face_dict:\n",
    "        face_dict[x[1]].append(x[0])\n",
    "    else:\n",
    "        face_dict[x[1]] = [x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:37:50.556745Z",
     "start_time": "2019-08-22T13:37:50.395094Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting consecutives\n",
    "cons_dict = {}\n",
    "for key,vals in trends.items():\n",
    "    key = HMS2sec(key)\n",
    "    for val in vals:\n",
    "        if val in cons_dict:\n",
    "            if (cons_dict[val][-1][-1]==prev_time):\n",
    "                cons_dict[val][-1].append(key)\n",
    "            else:\n",
    "                cons_dict[val].append([key])\n",
    "        else:\n",
    "            cons_dict[val] = [[key]]\n",
    "    prev_time = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:09:31.096370Z",
     "start_time": "2019-08-23T13:09:31.068449Z"
    }
   },
   "outputs": [],
   "source": [
    "face_intervals = {} #Dict containing exact timestamps of all occurences of an actor's face\n",
    "                    #in intervals specified by 'cons_dict'\n",
    "    \n",
    "for face,intervals in cons_dict.items():\n",
    "    face_intervals[face] = []\n",
    "    for times in intervals:\n",
    "        lb = min(x for x in face_dict[face] if x >= times[0]) #lower bound\n",
    "        ub = max(x for x in face_dict[face] if (x <= times[-1]+900)) #upper bound\n",
    "        face_intervals[face].append([x for x in face_dict[face] if (x>=lb and x<=ub)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:09.863441Z",
     "start_time": "2019-08-23T13:27:09.846163Z"
    }
   },
   "outputs": [],
   "source": [
    "#Converting the dict to a bunch of tuples of the form (face,each_interval)\n",
    "shows = [(face,times) for face in face_intervals.keys() for times in face_intervals[face]]\n",
    "shows = sorted(shows, key = lambda x: x[1][-1]) #Sorting face intervals by their order of ending time.\n",
    "shows = [list(x) for x in shows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:10.045635Z",
     "start_time": "2019-08-23T13:27:10.023315Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing too short\n",
    "min_len = 0.5*60 #In seconds #CHANGE\n",
    "shows = [x for x in shows if (x[1][-1] - x[1][0])>=min_len]\n",
    "#Will still be sorted by ending time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:10.165527Z",
     "start_time": "2019-08-23T13:27:10.156802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing intervals within intervals:\n",
    "show_intervals = [x[1] for x in shows]\n",
    "i = 0\n",
    "for x in range(len(shows)):\n",
    "    curr_interval = shows[i][1]\n",
    "    for x in show_intervals:\n",
    "        if(curr_interval[0]>x[0] and curr_interval[-1]<x[-1]):\n",
    "            del(shows[i])\n",
    "            i -= 1\n",
    "            break\n",
    "    i += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:10.372303Z",
     "start_time": "2019-08-23T13:27:10.342120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hosts: 0 & 32\n",
      "Original durations: 00:00:35 to 01:57:10 and 01:44:06 to 01:57:10\n",
      "Total duration: 784\n",
      "Overlap: 1.0\n",
      "Merging show 32 from 6246 to 7030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combining consecutive shows with very high overlap\n",
    "i = 0\n",
    "overlap_threshold = 0.75\n",
    "while(i<len(shows)-1):\n",
    "    diff = shows[i][1][-1] - shows[i+1][1][0]\n",
    "    #total = shows[i+1][1][-1] - shows[i][1][0]\n",
    "    short_show = min(shows[i][1][-1]-shows[i][1][0],shows[i+1][1][-1]-shows[i+1][1][0])\n",
    "    overlap = diff/short_show\n",
    "    if(overlap > overlap_threshold):\n",
    "        print('Hosts: {} & {}'.format(shows[i][0],shows[i+1][0]))\n",
    "        print('Original durations: {} to {} and {} to {}'.format(sec2HMS(shows[i][1][0]),sec2HMS(shows[i][1][-1]),sec2HMS(shows[i+1][1][0]),sec2HMS(shows[i+1][1][-1])))\n",
    "        print('Total duration: '+str(diff))\n",
    "        print('Overlap: '+str(overlap))\n",
    "        lb = min(shows[i][1][0],shows[i+1][1][0])\n",
    "        ub = max(shows[i][1][-1],shows[i+1][1][-1])\n",
    "        shows[i][0] = shows[i][0]+'&'+shows[i+1][0]\n",
    "        shows[i][1].extend(shows[i+1][1])\n",
    "        shows[i][1] = sorted(shows[i][1])\n",
    "        print('Merging show {} from {} to {}'.format(shows[i+1][0],shows[i+1][1][0],shows[i+1][1][-1]))\n",
    "        print()\n",
    "        del(shows[i+1])\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:10.508841Z",
     "start_time": "2019-08-23T13:27:10.487204Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing intervals which are overlapping between two shows.\n",
    "#Example: A - 01:00 to 10:00\n",
    "#         B - 09:00 to 12:00\n",
    "#         C - 10:00 to 20:00\n",
    "DOUBLE_OVERLAP_THRESHOLD = 0.85\n",
    "i=1\n",
    "while (i<len(shows)-1):\n",
    "    curr_show = len(shows[i][1]) #Length of current show\n",
    "    diff1 = len([x for x in shows[i][1] if x in range(shows[i-1][1][0],shows[i-1][1][-1])]) #Left side overlapping\n",
    "    overlap1 = diff1/curr_show\n",
    "    diff2 = len([x for x in shows[i][1] if x in range(shows[i+1][1][0],shows[i+1][1][-1])]) #Right side overlapping\n",
    "    overlap2 = diff2/curr_show\n",
    "    net_overlap = overlap1 + overlap2\n",
    "\n",
    "#actual algorithm\n",
    "    if(net_overlap > DOUBLE_OVERLAP_THRESHOLD):\n",
    "        print('Hosts: {} and {} and {}'.format(shows[i-1][0],shows[i][0],shows[i+1][0]))\n",
    "        print('Original durations: {} to {} and {} to {} and {} to {}'.format(sec2HMS(shows[i-1][1][0]),sec2HMS(shows[i-1][1][-1]),sec2HMS(shows[i][1][0]),sec2HMS(shows[i][1][-1]),sec2HMS(shows[i+1][1][0]),sec2HMS(shows[i+1][1][-1])))\n",
    "        #         print('Total duration: '+sec2HMS(diff))\n",
    "        #         print('Overlap: '+str(overlap))\n",
    "        print('Left overlap: {}'.format(overlap1))\n",
    "        print('Right overlap: {}'.format(overlap2))\n",
    "        print('Net overlap: {}'.format(net_overlap))\n",
    "        print()\n",
    "        del(shows[i])\n",
    "    else:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:10.662879Z",
     "start_time": "2019-08-23T13:27:10.654027Z"
    }
   },
   "outputs": [],
   "source": [
    "# #july 4\n",
    "# #The Kagan Problem solution - TODO - probably not feasible - make it so that it considers every case as TEASER case but not the Kagan problem case\n",
    "# shows_refined = [[shows[-1][0],[shows[-1][1]]]]\n",
    "# for curr in range(len(shows)-2,-1,-1):\n",
    "#     nxt = curr+1\n",
    "# #     if not (shows[curr][1]): #If current boundary was completely removed(in the previous iteration), we can skip it.\n",
    "# #         continue\n",
    "    \n",
    "# #     if not(shows[nxt][1]) or not(shows[curr][1]):\n",
    "# #         continue\n",
    "        \n",
    "#     nxt_lb   = shows[nxt][1][0]\n",
    "#     curr_ub  = shows[curr][1][-1]\n",
    "#     if(curr_ub>=nxt_lb): \n",
    "#         nxt_lb2  = max([x for x in shows[nxt][1] if x<=curr_ub])\n",
    "#         curr_ub0 = min([x for x in shows[curr][1] if x>=nxt_lb])\n",
    "#         #TODO: Double sided\n",
    "#         shows_refined[-1][1] = [[x for x in shows[nxt][1] if x<=nxt_lb2],[x for x in shows[nxt][1] if x>nxt_lb2]]\n",
    "#         shows_refined.append([shows[curr][0],[[x for x in shows[curr][1] if x<=curr_ub0],[x for x in shows[curr][1] if x>curr_ub0]]]) #Only taking values not overlapping with the next show\n",
    "#     else:\n",
    "#         shows_refined.append([shows[curr][0],[shows[curr][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:10.847921Z",
     "start_time": "2019-08-23T13:27:10.825212Z"
    }
   },
   "outputs": [],
   "source": [
    "shows_refined = [shows[0]]\n",
    "prev_show = shows[0]\n",
    "for show in shows[1:]:\n",
    "    shows_refined.append([show[0], [x for x in show[1] if x>=prev_show[1][-1]] ])\n",
    "    prev_show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:41:00.933456Z",
     "start_time": "2019-08-23T13:41:00.913090Z"
    }
   },
   "outputs": [],
   "source": [
    "shows = [Show(str(x[0]),x[1][0],x[1][-1]) for x in shows_refined]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows are done, now finding the hosts' names of each show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:41:01.492313Z",
     "start_time": "2019-08-23T13:41:01.484425Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# 1. Do the celeb recognition thing and replace hosts of every Show with that celeb\n",
    "#  * If face is a good majority -> make it host name, leave the top 5 predictions in INF\n",
    "#FIX KNN's N or the epsilon value\n",
    "#Remove unconfident faces - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.kneighbors https://medium.com/@mohtedibf/in-depth-parameter-tuning-for-knn-4c0de485baf6\n",
    "# 2. Test on a few videos\n",
    "# 3. Cut out videos from this information\n",
    "# 4. Use .txt3 to make .txt for each of these cuts(3&4 in the same for loop)\n",
    "# 5. clear TODOs and prints, make python scripts with input format\n",
    "# 6. put on github and clone on the singularity, make video input format on singularity, check output formats with Steen and go back to 3.\n",
    "# 7. documentation/ blogs from slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:41:16.929356Z",
     "start_time": "2019-08-23T13:41:01.807511Z"
    }
   },
   "outputs": [],
   "source": [
    "for show in shows:\n",
    "    hosts = show.hosts.split('&') #getting list of hosts of the show\n",
    "    hosts = sorted(hosts, key = lambda x: len(face_encodings[int(x)]), reverse=True) #Most occuring anchor is taken as the main anchor\n",
    "    for i in range(len(hosts)):\n",
    "        host = hosts[i]\n",
    "        host_encodings = face_encodings[int(host)]      #Getting all encodings of this host's face\n",
    "        host_prob_names = Counter(list(encoding2name(host_encodings))) #Getting predictions of all faces\n",
    "        hosts[i] = [(x,y/len(host_encodings)) for x,y in host_prob_names.most_common()] #sorting the predictions by their frequency\n",
    "        \n",
    "    show.hosts = hosts\n",
    "#     show.hosts = sorted(hosts, key = lambda x: sum(y for _,y in x))\n",
    "# (neigh.kneighbors(face_encodings[0]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T13:27:34.532009Z",
     "start_time": "2019-08-23T13:27:34.529874Z"
    }
   },
   "outputs": [],
   "source": [
    "# shows is a list of all 'Show's. Each Show's hosts attribute is a list of all predictions for each celeb.\n",
    "# Each celeb has its own list having the predictions sorted according to frequency\n",
    "# TODO: check if output shows are sorted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:38:17.058159Z",
     "start_time": "2019-08-22T13:38:16.554261Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extracting video metadata\n",
    "vcap = cv2.VideoCapture(vid_path)\n",
    "vid_width, vid_height = int(vcap.get(3)), int(vcap.get(4))\n",
    "vcap.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
    "vid_duration = int(vcap.get(cv2.CAP_PROP_POS_MSEC)/1000)\n",
    "cv2.destroyAllWindows()\n",
    "vcap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T07:52:00.444436Z",
     "start_time": "2019-08-23T07:52:00.395259Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = os.path.splitext(os.path.basename(vid_path))[0]\n",
    "attributes = filename.split('_')\n",
    "pulldate, barcode = attributes[0], attributes[3]\n",
    "vid_txt3_path = os.path.splitext(vid_path)[0]+'.txt3'\n",
    "txt3_subtitles = None\n",
    "\n",
    "#common headers for all cuts - default values (to be used if this column is not present in the txt3)\n",
    "OVD = 'OVD|'+filename+'.mp4'  \n",
    "OID = 'OID|'\n",
    "COL = 'COL|Communication Studies Archive, UCLA'\n",
    "SRC = 'SRC|Rosenthal Collection, UCLA'\n",
    "LAN = 'LAN|ENG'\n",
    "LBT = 'LBT|'\n",
    "\n",
    "if os.path.exists(vid_txt3_path):\n",
    "    txt3_lines = open(vid_txt3_path, 'r').read().splitlines()\n",
    "    \n",
    "    for i in range(len(txt3_lines)):\n",
    "        if txt3_lines[i][3]!='|': #Header lines end\n",
    "            txt3_headers = txt3_lines[:i]\n",
    "            txt3_subtitles = txt3_lines[i:] #Subtitles' lines start here\n",
    "            break\n",
    "            \n",
    "    for header in txt3_headers:\n",
    "        if header[:3]=='TOP':\n",
    "            OVD = 'OVD|'+header[4:]\n",
    "        elif header[:3]=='UID':\n",
    "            OID = 'OID|'+header[4:]\n",
    "        elif header[:3]=='COL':\n",
    "            COL = header\n",
    "        elif header[:3]=='SRC':\n",
    "            SRC = header\n",
    "        elif header[:3]=='LAN':\n",
    "            LAN = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T14:05:10.893444Z",
     "start_time": "2019-08-23T14:05:10.854038Z"
    }
   },
   "outputs": [],
   "source": [
    "#Cutting shows from the main video + making a .txt file for each\n",
    "for n_show, show in enumerate(shows):\n",
    "    \n",
    "    channel = 'unknown-channel' #until the work with IMDb is done\n",
    "    channel = channel.replace(' ', '_')\n",
    "    \n",
    "    main_host = show.hosts[0][0]\n",
    "    if main_host[1] > 0.45: #If majority predictions are of the same person\n",
    "        host_name = main_host[0]\n",
    "    else:\n",
    "        host_name = 'unknown-host'\n",
    "#     host_name = show.hosts[0][0][0]\n",
    "    host_name = host_name.replace(' ', '_')\n",
    "    cut_filename = '_'.join((pulldate, barcode, '-'.join((str(n_show+1), str(len(shows)))), channel, host_name))\n",
    "    cut_path = os.path.join(output_path, cut_filename)\n",
    "    cut_starttime = (int(max(0, show.start_time - 60))) #using a buffer of 1 minute\n",
    "                            \n",
    "    if n_show==len(shows)-1: #last show                            \n",
    "        cut_endtime = (int(min(show.end_time + 60, vid_duration)))\n",
    "    else:\n",
    "        cut_endtime = (int(shows[n_show+1].start_time)) #till the start of next show\n",
    "    \n",
    "    cut_duration = sec2HMS(cut_endtime - cut_starttime)\n",
    "    cut_starttime = sec2HMS(cut_starttime)\n",
    "    \n",
    "     ffmpeg_command = 'ffmpeg -ss {} -t {} -i {} -vcodec copy -acodec copy {}.mp4'.format(cut_starttime, cut_duration, vid_path, cut_path)\n",
    "     os.system(ffmpeg_command)\n",
    "    \n",
    "    TOP = 'TOP|'+cut_filename+'.mp4'\n",
    "    UID = 'UID|' #TODO: Generate UUID in the cluster\n",
    "    TTL = 'TTL|'\n",
    "    PID = 'PID|'\n",
    "    CMT = 'CMT|'\n",
    "    INF = 'INF|'\n",
    "    for i, host in enumerate(show.hosts):\n",
    "        INF += 'probable_host'+str(i+1)+':'+'_'.join([pred[0].replace(' ','-') for pred in host][:5])+'_'\n",
    "    INF = INF[:-1]        \n",
    "    DUR = 'DUR|'+cut_duration\n",
    "    TMS = 'TMS|'+cut_starttime+'-'+sec2HMS(cut_endtime)\n",
    "    VID = 'VID|{}x{}'.format(vid_width, vid_height)\n",
    "    \n",
    "    #initializing with headers\n",
    "    cut_txt_lines = [TOP, COL, UID, SRC, TTL, PID, CMT, DUR, VID, LAN, LBT, OVD, OID, TMS, INF] \n",
    "    \n",
    "    sub_starttime = pulldate.replace('-','') + cut_starttime.replace(':','')\n",
    "    sub_endtime = pulldate.replace('-','') + sec2HMS(cut_endtime).replace(':','')\n",
    "    \n",
    "    \n",
    "    if txt3_subtitles:\n",
    "        for sub_idx in range(len(txt3_subtitles)): #TODO: maybe make the starting time 0 for each?\n",
    "            curr_sub = txt3_subtitles[sub_idx]\n",
    "            \n",
    "            if curr_sub[:14] >= sub_starttime:\n",
    "                if curr_sub[:14] <= sub_endtime:\n",
    "                    cut_txt_lines.append(txt3_subtitles[sub_idx])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    with open(cut_path+'.txt', 'w') as f:\n",
    "        for line in cut_txt_lines:\n",
    "            f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T14:05:11.330341Z",
     "start_time": "2019-08-23T14:05:11.034638Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/output_cuts/2006-01-02_00001057_1-1_unknown-channel_Daryn_Kagan.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
